{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16a4c27-14e5-4824-a2d8-92a287ca2b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 15:13:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              67W / 500W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d895ce-6a1a-4ae3-b5d1-4881f4eeca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DistilBertTokenizer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import wandb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72588ce9-306e-4319-91c1-317579cff1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ad7d8e-6df4-41ba-831b-b8de171560f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutBert2Bert(AutoModelForMultipleChoice):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Set dropout rate\n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        # Move model to device (e.g., GPU)\n",
    "        self.to(device)\n",
    "    \n",
    "    def _set_dropout_rate(self, module):\n",
    "        \"\"\"Helper function to set dropout rate for all Dropout layers.\"\"\"\n",
    "        for submodule in module.modules():\n",
    "            if isinstance(submodule, torch.nn.Dropout):\n",
    "                submodule.p = self.dropout_rate\n",
    "                submodule.train()  # Make sure dropout is active in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44065edc-9e1e-440b-95e9-4b7711047268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=False):\n",
    "        try:\n",
    "            inputs = {k: v.to(model.device) if hasattr(v, 'to') else v \n",
    "             for k, v in inputs.items()}\n",
    "            labels = inputs.pop(\"labels\").to(model.device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits  # Get the last token's logits\n",
    "            # Compute cross entropy loss\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits,\n",
    "                labels,\n",
    "                ignore_index=-100\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                valid_mask = labels != -100\n",
    "                accuracy = (predictions[valid_mask] == labels[valid_mask]).float().mean()\n",
    "\n",
    "                if self.args.logging_steps > 0 and self.state.global_step % self.args.logging_steps == 0:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": loss.item(),\n",
    "                        \"train_accuracy\": accuracy.item(),\n",
    "                        \"train_step\": self.state.global_step,\n",
    "                        \"train_epoch\": self.state.epoch,\n",
    "                    })\n",
    "\n",
    "            # Clear unnecessary tensors\n",
    "            del predictions, valid_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Loss computation error: {str(e)}\")\n",
    "            raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d133eea5-8547-41c5-8f4a-623279c6a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_tokenize_dataset(file_path, tokenizer):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "\n",
    "        formatted_data = []\n",
    "        for item in tqdm(data, desc=\"Processing dataset\"):\n",
    "            question = item[\"question\"]\n",
    "            options = [item[\"options\"][key] for key in [\"A\", \"B\", \"C\", \"D\", \"E\"]]\n",
    "\n",
    "            # Tokenize question with each option\n",
    "            tokenized_options = tokenizer(\n",
    "                [f\"{question} {opt}\" for opt in options],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            label = ord(item[\"answer_idx\"]) - ord('A')\n",
    "            formatted_data.append({\n",
    "                \"input_ids\": tokenized_options[\"input_ids\"],\n",
    "                \"attention_mask\": tokenized_options[\"attention_mask\"],\n",
    "                \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            })\n",
    "\n",
    "        return Dataset.from_list(formatted_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Dataset preparation error: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a6ccc6a-064c-432d-9737-46601b37d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        batch = {}\n",
    "        \n",
    "        # Process input_ids and attention_mask\n",
    "        batch_encoding = self.tokenizer.pad(\n",
    "            [{\"input_ids\": ex[\"input_ids\"], \"attention_mask\": ex[\"attention_mask\"]} for ex in examples],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        batch[\"input_ids\"] = batch_encoding[\"input_ids\"]\n",
    "        batch[\"attention_mask\"] = batch_encoding[\"attention_mask\"]\n",
    "        \n",
    "        # Process labels\n",
    "        if \"labels\" in examples[0]:\n",
    "            batch[\"labels\"] = torch.tensor([ex[\"labels\"] for ex in examples], dtype=torch.long)\n",
    "            \n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7352efcf-df6f-4a99-9243-36cf1fbc076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples: Dict, tokenizer, max_length: int = 512):\n",
    "    \"\"\"Tokenize the input data.\"\"\"\n",
    "    prompts = [f\"{instruction}\\n{response}\" \n",
    "              for instruction, response in zip(examples[\"instruction\"], examples[\"response\"])]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Create labels (same as input_ids for causal language modeling)\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(tokenized[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(tokenized[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dbf700e-fb54-4b2b-b39d-986a8f86b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_and_tokenizer():\n",
    "    \"\"\"Initialize the BERT2BERT model and tokenizer.\"\"\"\n",
    "    try:\n",
    "        # Load the tokenizer for BERT\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\",\n",
    "            use_fast=False,\n",
    "            padding_side=\"right\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Load the BERT2BERT encoder-decoder model\n",
    "        model = AutoModelForMultipleChoice.from_pretrained(\n",
    "            \"bert-base-uncased\", \n",
    "        )\n",
    "\n",
    "        # Add pad_token if necessary (BERT models often use [PAD] token)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "            model.resize_token_embeddings(len(tokenizer))  # Adjust model's embedding layer\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model/tokenizer: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fd07f4-e2cb-4b99-b02e-6056f1a64675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and tokenizer\n",
    "model, tokenizer = prepare_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "745ebee0-e6e1-4919-933a-a584d946fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 109483009\n",
      "Trainable parameters: 109483009\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {total_params - trainable_params}\")\n",
    "\n",
    "# Example usage:\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a19afd-4ab3-45e2-9b5f-9543e8df30d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 10178/10178 [01:49<00:00, 93.05it/s] \n"
     ]
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = prepare_and_tokenize_dataset(\"MedQADataset/US/train.jsonl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b927355-2432-4fa7-93e8-763e0128d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-medical-qa-\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecc54029-068a-4b11-ac54-066ac9bd50bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ntfhymms) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-tuning-train-val-only</strong> at: <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/ntfhymms' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/ntfhymms</a><br/> View project at: <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241207_151406-ntfhymms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ntfhymms). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shah.harsh8/wandb/run-20241207_151614-rj1v3gkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm' target=\"_blank\">fine-tuning-train-val-only</a></strong> to <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b4357433820>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Weights and Biases (wandb)\n",
    "wandb.init(\n",
    "    project=\"bert-medical-qa-finetune\",\n",
    "    name=\"fine-tuning-train-val-only\",\n",
    "    config={\n",
    "        \"model\": \"bert-base-uncased\",  # Change model name for BERT2BERT\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": 2e-4\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9425facd-fb83-4d05-acfd-d1e9fab07a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-b8d7bfca4ac6>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer with CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # BERT2BERT model\n",
    "    args=training_args,  # Training arguments defined earlier\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    tokenizer=tokenizer,  # BERT tokenizer\n",
    "    data_collator=CustomDataCollator(tokenizer),  # Custom data collator for BERT2BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d41bb58-175f-4683-8ed6-2c63fd812cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [636/636 30:36, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.610500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.607900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.608100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.607300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.612500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=636, training_loss=1.61302997631097, metrics={'train_runtime': 1840.053, 'train_samples_per_second': 11.063, 'train_steps_per_second': 0.346, 'total_flos': 2.676604733807616e+16, 'train_loss': 1.61302997631097, 'epoch': 1.9984289080911233})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12fb35b6-d11c-44d2-9288-2433fc50e67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-medical-final/tokenizer_config.json',\n",
       " './bert-medical-final/special_tokens_map.json',\n",
       " './bert-medical-final/vocab.txt',\n",
       " './bert-medical-final/added_tokens.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bert-medical-final\")\n",
    "tokenizer.save_pretrained(\"./bert-medical-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871e640e-8d49-4c3d-9f77-3ba7704a8d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▅▃▃▃▂▂▂▂▄▂▂▂▃▃▄▃▂▃▃▁▁▁▂▃▁▄▁▁▂▁▁▁▂▂▁▁▁▂▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▄▅▇▇███▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▁▆▃▃▂▅▄▆▄▄▂▄▃▃▃▃▃▄▃▃▃▃▄▄▃▄▅▂▃▂▂▅▃▂▄▃▄▄</td></tr><tr><td>train_accuracy</td><td>▄▂▄▅▅▂▄▅▂▄▇█▅▂▄▅▄▄▂▂▄▂▄▄▂▄▄▁▄▅▄▂▂▁▂▂▄▄▄▂</td></tr><tr><td>train_epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>▁█▆▆▆▅▃▃▅▃▂▆▁▇▅▃▄▃█▅▇▅▃▆▅▅▅▃▅▆▃▆▆▅▇▄▅▇▄▃</td></tr><tr><td>train_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2.676604733807616e+16</td></tr><tr><td>train/epoch</td><td>1.99843</td></tr><tr><td>train/global_step</td><td>636</td></tr><tr><td>train/grad_norm</td><td>3.67541</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6125</td></tr><tr><td>train_accuracy</td><td>0.375</td></tr><tr><td>train_epoch</td><td>1.97958</td></tr><tr><td>train_loss</td><td>1.61303</td></tr><tr><td>train_runtime</td><td>1840.053</td></tr><tr><td>train_samples_per_second</td><td>11.063</td></tr><tr><td>train_step</td><td>630</td></tr><tr><td>train_steps_per_second</td><td>0.346</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-tuning-train-val-only</strong> at: <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune/runs/rj1v3gkm</a><br/> View project at: <a href='https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune' target=\"_blank\">https://wandb.ai/harsh012001-northeastern-university/bert-medical-qa-finetune</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241207_151614-rj1v3gkm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
